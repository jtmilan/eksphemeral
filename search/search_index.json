{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The EKS Ephemeral Cluster Manager \u00b6 Note This is a service for development and test environments. It is not an official AWS offering, use at your own risk. Managing Amazon EKS clusters for development and test environments manually is boring. You have to wait until they're provisioned and then have to remember to tear them down again, in order to minimize costs. How about automating these steps? Meet EKSphemeral, the simple manager for ephemeral EKS clusters, allowing you to launch EKS clusters that auto-tear down after some time, and you can also prolong their lifetime if you want to continue to use them. If you like, you can have a look at a video walkthrough , before you try it out yourself. You can either use the EKSphemeral UI : Or the EKSphemeral CLI : Install \u00b6 In order to use EKSphemeral you need to have jq ( install ) and the aws CLI ( install ) installed. All other dependencies, such as the Fargate CLI will be installed automatically, if not present on the system. Warning Make sure to set the respective environment variables as shown here before you proceed. Most install issue come from not all environment variables set. The following environment variables need to be set so that the install process knows where the dependencies are and which S3 bucket to use for the control plane ( EKSPHEMERAL_SVC_BUCKET ) and where to put the cluster metadata ( EKSPHEMERAL_CLUSTERMETA_BUCKET ), for example: 1 2 3 $ export EKSPHEMERAL_HOME = ~/eksp $ export EKSPHEMERAL_SVC_BUCKET = eks-svc $ export EKSPHEMERAL_CLUSTERMETA_BUCKET = eks-cluster-meta Optionally, in order to receive email notifications about cluster creation and destruction, you need to set the EKSPHEMERAL_EMAIL_FROM environment variable, for example: 1 $ export EKSPHEMERAL_EMAIL_FROM = hausenbl+eksphemeral@amazon.com Note In addition to setting the EKSPHEMERAL_EMAIL_FROM environment variable, you MUST verify both the source email, that is, the address you provide in EKSPHEMERAL_EMAIL_FROM as well as the target email address (in the owner field of the cluster spec, see below for details) in the EU (Ireland) eu-west-1 region. We're now in the position to install EKSphemeral with a single command, here shown for an install below your home directory: 1 $ curl -sL http://get.eksphemeral.info/install.sh | sudo --preserve-env bash This process can take several minutes. After this, EKSphemeral is installed in your AWS environment as well as the CLI is locally available. Learn more about what exactly is created and running as part of the install process by perusing the EKSphemeral architecture . Use \u00b6 You can create, inspect, and prolong the lifetime of a cluster with the CLI or, if you prefer a visual interface check out the local EKSphemeral UI proxy, which requires Docker.","title":"Home"},{"location":"#the-eks-ephemeral-cluster-manager","text":"Note This is a service for development and test environments. It is not an official AWS offering, use at your own risk. Managing Amazon EKS clusters for development and test environments manually is boring. You have to wait until they're provisioned and then have to remember to tear them down again, in order to minimize costs. How about automating these steps? Meet EKSphemeral, the simple manager for ephemeral EKS clusters, allowing you to launch EKS clusters that auto-tear down after some time, and you can also prolong their lifetime if you want to continue to use them. If you like, you can have a look at a video walkthrough , before you try it out yourself. You can either use the EKSphemeral UI : Or the EKSphemeral CLI :","title":"The EKS Ephemeral Cluster Manager"},{"location":"#install","text":"In order to use EKSphemeral you need to have jq ( install ) and the aws CLI ( install ) installed. All other dependencies, such as the Fargate CLI will be installed automatically, if not present on the system. Warning Make sure to set the respective environment variables as shown here before you proceed. Most install issue come from not all environment variables set. The following environment variables need to be set so that the install process knows where the dependencies are and which S3 bucket to use for the control plane ( EKSPHEMERAL_SVC_BUCKET ) and where to put the cluster metadata ( EKSPHEMERAL_CLUSTERMETA_BUCKET ), for example: 1 2 3 $ export EKSPHEMERAL_HOME = ~/eksp $ export EKSPHEMERAL_SVC_BUCKET = eks-svc $ export EKSPHEMERAL_CLUSTERMETA_BUCKET = eks-cluster-meta Optionally, in order to receive email notifications about cluster creation and destruction, you need to set the EKSPHEMERAL_EMAIL_FROM environment variable, for example: 1 $ export EKSPHEMERAL_EMAIL_FROM = hausenbl+eksphemeral@amazon.com Note In addition to setting the EKSPHEMERAL_EMAIL_FROM environment variable, you MUST verify both the source email, that is, the address you provide in EKSPHEMERAL_EMAIL_FROM as well as the target email address (in the owner field of the cluster spec, see below for details) in the EU (Ireland) eu-west-1 region. We're now in the position to install EKSphemeral with a single command, here shown for an install below your home directory: 1 $ curl -sL http://get.eksphemeral.info/install.sh | sudo --preserve-env bash This process can take several minutes. After this, EKSphemeral is installed in your AWS environment as well as the CLI is locally available. Learn more about what exactly is created and running as part of the install process by perusing the EKSphemeral architecture .","title":"Install"},{"location":"#use","text":"You can create, inspect, and prolong the lifetime of a cluster with the CLI or, if you prefer a visual interface check out the local EKSphemeral UI proxy, which requires Docker.","title":"Use"},{"location":"arch/","text":"Architecture \u00b6 EKSphemeral has a control plane implemented in an AWS Lambda/Amazon S3 combo, and as its data plane it is using eksctl running in AWS Fargate. The architecture looks as follows: With eksp install you provisions EKSphemeral's control plane (Lambda+S3). Whenever you want to provision a throwaway EKS cluster, use eksp create . It will do two things: Provision the cluster using eksctl running in Fargate, and when that is completed, Create an cluster spec entry in S3, via the /create endpoint of EKSphemeral's HTTP API. Every five minutes, a CloudWatch event triggers the execution of another Lambda function called DestroyClusterFunc , which notifies the owners of clusters that are about to expire (send an email up to 5 minutes before the cluster is destroyed), and when the time comes, it tears the cluster down. Once the EKS cluster is provisioned and the Kubernetes context is configured you can use your cluster. You can use eksp list (via the /status endpoint) at any time to list managed clusters. If you want to keep your cluster around longer, use eksp prolong (via the /prolong endpoint) to extend its lifetime. Last but not least, if you want to get rid of EKSphemeral, use the eksp uninstall , removing all cluster specs in the S3 bucket and deleting all Lambda functions.","title":"Architecture"},{"location":"arch/#architecture","text":"EKSphemeral has a control plane implemented in an AWS Lambda/Amazon S3 combo, and as its data plane it is using eksctl running in AWS Fargate. The architecture looks as follows: With eksp install you provisions EKSphemeral's control plane (Lambda+S3). Whenever you want to provision a throwaway EKS cluster, use eksp create . It will do two things: Provision the cluster using eksctl running in Fargate, and when that is completed, Create an cluster spec entry in S3, via the /create endpoint of EKSphemeral's HTTP API. Every five minutes, a CloudWatch event triggers the execution of another Lambda function called DestroyClusterFunc , which notifies the owners of clusters that are about to expire (send an email up to 5 minutes before the cluster is destroyed), and when the time comes, it tears the cluster down. Once the EKS cluster is provisioned and the Kubernetes context is configured you can use your cluster. You can use eksp list (via the /status endpoint) at any time to list managed clusters. If you want to keep your cluster around longer, use eksp prolong (via the /prolong endpoint) to extend its lifetime. Last but not least, if you want to get rid of EKSphemeral, use the eksp uninstall , removing all cluster specs in the S3 bucket and deleting all Lambda functions.","title":"Architecture"},{"location":"cli/","text":"The EKSphemeral CLI \u00b6 Info Currently, the CLI binaries are available for both macOS and Linux platforms. You can create, inspect, and prolong the lifetime of a cluster with the CLI as shown in the following. Manual install \u00b6 Note You usually don't need to install the CLI manually, it should have been set up with the overall install. However, in cases where you want to access EKSphemeral from a machine other than the one you set it up originally or the CLI has been removed by someone or something, follow the steps here. To manually install the binary CLI, for example on macOS, do: 1 2 3 $ curl -sL https://github.com/mhausenblas/eksphemeral/releases/latest/download/eksp-macos -o eksp $ chmod +x eksp $ sudo mv ./eksp /usr/local/bin Now, let's check if there are already clusters are managed by EKSphemeral: 1 2 $ eksp list No clusters found Since we just installed EKSphemeral, there are no clusters, yet. Let's change that. Create clusters \u00b6 The CLI allows you to create ephemeral EKS clusters with a single command, based on a simple JSON cluster spec file. Basics \u00b6 Let's create a cluster named mh9-eksp , with three worker nodes, using Kubernetes version 1.12, with a 150 min timeout. First, create a file cluster-spec.json with the following content: 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"\" , \"name\" : \"mh9-eksp\" , \"numworkers\" : 3 , \"kubeversion\" : \"1.12\" , \"timeout\" : 150 , \"ttl\" : 150 , \"owner\" : \"hausenbl+notif@amazon.com\" , \"created\" : \"\" } Now you can use the create command like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ eksp create cluster-spec.json Trying to create a new ephemeral cluster ... ... using cluster spec cluster-spec.json Seems you have set 'us-east-2' as the target region, using this for all following operations I will now provision the EKS cluster mh9-eksp using AWS Fargate: [ i ] Running task eksctl Waiting for EKS cluster provisioning to complete. Allow some 15 min to complete, checking status every minute: ......... Successfully created data plane for cluster mh9-eksp using AWS Fargate and now moving on to the control plane in AWS Lambda and S3 ... Successfully created control plane entry for cluster mh9-eksp via AWS Lambda and Amazon S3 ... Now moving on to configure kubectl to point to your EKS cluster: Updated context arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp in /Users/hausenbl/.kube/config Your EKS cluster is now set up and configured: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp Note that it still can take up to 5 min until the worker nodes are available, check with the following command until you don 't see the ' No resources found. ' message anymore: kubectl get nodes Note If no cluster spec is provided, a default cluster spec will be used along with the first security group of the default VPC. Once the cluster is ready and you've verified your email addresses you should get a notification that looks something like the following: The same is true at least five minutes before the cluster shuts down. Tip Above implicitly uses the base image. If you want a few more things installed, such as the Kubernetes dashboard, ArgoCD, and App Mesh, use the eksctl:deluxe image as shown in the following. Advanced cluster creation \u00b6 You can also use the deluxe image, available via Quay.io , to create an ephemeral cluster with the Kubernetes Dashboard, ArgoCD , and AWS App Mesh (incl. Prometheus and Grafana) pre-installed. Do the following to provision an ephemeral deluxe cluster: 1 2 3 4 5 6 7 8 9 10 11 $ cat /tmp/eks-deluxe.json { \"id\" : \"\" , \"name\" : \"mh9-deluxe\" , \"numworkers\" : 2 , \"kubeversion\" : \"1.13\" , \"timeout\" : 1440 , \"ttl\" : 1440 , \"owner\" : \"hausenbl+notif@amazon.com\" , \"created\" : \"\" } And then: 1 2 $ EKSPHEMERAL_EKSCTL_IMG = deluxe eksp create /tmp/eks-deluxe.json ... This takes some 15 min and after that, to access the Kubernetes dashboard we need to 1. proxy the UI locally, and 2. sort out the access control bits. First, launch the proxy to forward traffic to you local environment: 1 $ kubectl proxy Next, let's sort out the access control bits: First, create a service account eks-admin : 1 $ kubectl -n kube-system create sa eks-admin ... and give it cluster admin rights ... 1 2 3 $ kubectl create clusterrolebinding eks-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = kube-system:eks-admin ... and finally, get the token for logging into the dashboard: 1 2 3 $ kubectl -n kube-system describe secret \\ $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) | grep ^token token: e****************************************************************************************************************g Finally, got to the dashboard and use the value of token from the last command to log into the dashboard: Once you've completed the login, select the \"Namespace\" tab, left-hand side to check if all is as it should be, compare with the following screen shot: To access the ArgoCD UI , do: 1 $ kubectl port-forward svc/argocd-server -n argocd 8080 :443 And now execute: 1 2 $ kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd-server-7c7c748648-nlrwq Finally, open localhost:8080 and log in with username admin and password using the pod name from the previous step, in my case argocd-server-7c7c748648-nlrwq : Once you've completed the login, you are ready to configure your first app deployment, GitOps style: That's it. Now let's see how we can introspect and manipulate EKSphemeral clusters. List clusters \u00b6 Next, let's check what clusters are managed by EKSphemeral: 1 2 3 $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 45 min 42 min hausenbl+notif@amazon.com Here, we get an tabular rendering of the clusters. We can use a cluster ID as follows to look up the spec of a particular cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ eksp list e90379cf-ee0a-49c7-8f82-1660760d6bb5 ID: e90379cf-ee0a-49c7-8f82-1660760d6bb5 Name: mh9-eksp Kubernetes: v1.12 Worker nodes: 2 Timeout: 45 min TTL: 38 min Owner: hausenbl+notif@amazon.com Details: Status: ACTIVE Endpoint: https://A377918A0CA6D8BE793FF8BEC88964FE.sk1.us-east-2.eks.amazonaws.com Platform version: eks.2 VPC config: private access: false, public access: true IAM role: arn:aws:iam::661776721573:role/eksctl-mh9-eksp-cluster-ServiceRole-1HT8OAOGNNY2Y Prolong cluster lifetime \u00b6 When you get a notification that one of your clusters is about to shut down or really at any time before it shuts down, you can prolong the cluster lifetime using the eksp prolong command. Let's say we want to keep the cluster with the ID e90379cf-ee0a-49c7-8f82-1660760d6bb5 around for 13 min longer. Here's what you would do: 1 2 3 4 $ eksp prolong e90379cf-ee0a-49c7-8f82-1660760d6bb5 13 Trying to set the TTL of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 to 13 minutes, starting now Successfully prolonged the lifetime of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 for 13 minutes. Now let's check: 1 2 3 $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 13 min 13 min hausenbl+notif@amazon.com Note The prolong command updates the timeout field of your cluster spec, that is, the cluster TTL is counted from the moment you issue the prolong command, taking the remaining cluster runtime into account. Uninstall \u00b6 To uninstall EKSphemeral, use the following command. This will remove the control plane elements, that is, delete the Lambda functions and remove all cluster specs from the EKSPHEMERAL_CLUSTERMETA_BUCKET S3 bucket: 1 2 3 4 5 6 7 8 9 10 11 $ eksp uninstall Trying to uninstall EKSphemeral ... Taking down the EKSphemeral control plane, this might take a few minutes ... aws s3 rm s3://eks-cluster-meta --recursive aws cloudformation delete-stack --stack-name eksp Tear-down will complete within some 5 min. You can check the status manually, if you like, using 'make status' in the svc/ directory. Once you see a message saying something like 'Stack with id eksp does not exist' you know for sure it ' s gone : ) Thanks for using EKSphemeral and hope to see ya soon ; ) Warning The service code bucket and the cluster metadata bucket are still around after the uninstall command has completed. You can either manually delete them or keep them around, to reuse them later. This concludes the CLI walkthrough.","title":"CLI"},{"location":"cli/#the-eksphemeral-cli","text":"Info Currently, the CLI binaries are available for both macOS and Linux platforms. You can create, inspect, and prolong the lifetime of a cluster with the CLI as shown in the following.","title":"The EKSphemeral CLI"},{"location":"cli/#manual-install","text":"Note You usually don't need to install the CLI manually, it should have been set up with the overall install. However, in cases where you want to access EKSphemeral from a machine other than the one you set it up originally or the CLI has been removed by someone or something, follow the steps here. To manually install the binary CLI, for example on macOS, do: 1 2 3 $ curl -sL https://github.com/mhausenblas/eksphemeral/releases/latest/download/eksp-macos -o eksp $ chmod +x eksp $ sudo mv ./eksp /usr/local/bin Now, let's check if there are already clusters are managed by EKSphemeral: 1 2 $ eksp list No clusters found Since we just installed EKSphemeral, there are no clusters, yet. Let's change that.","title":"Manual install"},{"location":"cli/#create-clusters","text":"The CLI allows you to create ephemeral EKS clusters with a single command, based on a simple JSON cluster spec file.","title":"Create clusters"},{"location":"cli/#basics","text":"Let's create a cluster named mh9-eksp , with three worker nodes, using Kubernetes version 1.12, with a 150 min timeout. First, create a file cluster-spec.json with the following content: 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"\" , \"name\" : \"mh9-eksp\" , \"numworkers\" : 3 , \"kubeversion\" : \"1.12\" , \"timeout\" : 150 , \"ttl\" : 150 , \"owner\" : \"hausenbl+notif@amazon.com\" , \"created\" : \"\" } Now you can use the create command like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ eksp create cluster-spec.json Trying to create a new ephemeral cluster ... ... using cluster spec cluster-spec.json Seems you have set 'us-east-2' as the target region, using this for all following operations I will now provision the EKS cluster mh9-eksp using AWS Fargate: [ i ] Running task eksctl Waiting for EKS cluster provisioning to complete. Allow some 15 min to complete, checking status every minute: ......... Successfully created data plane for cluster mh9-eksp using AWS Fargate and now moving on to the control plane in AWS Lambda and S3 ... Successfully created control plane entry for cluster mh9-eksp via AWS Lambda and Amazon S3 ... Now moving on to configure kubectl to point to your EKS cluster: Updated context arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp in /Users/hausenbl/.kube/config Your EKS cluster is now set up and configured: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp Note that it still can take up to 5 min until the worker nodes are available, check with the following command until you don 't see the ' No resources found. ' message anymore: kubectl get nodes Note If no cluster spec is provided, a default cluster spec will be used along with the first security group of the default VPC. Once the cluster is ready and you've verified your email addresses you should get a notification that looks something like the following: The same is true at least five minutes before the cluster shuts down. Tip Above implicitly uses the base image. If you want a few more things installed, such as the Kubernetes dashboard, ArgoCD, and App Mesh, use the eksctl:deluxe image as shown in the following.","title":"Basics"},{"location":"cli/#advanced-cluster-creation","text":"You can also use the deluxe image, available via Quay.io , to create an ephemeral cluster with the Kubernetes Dashboard, ArgoCD , and AWS App Mesh (incl. Prometheus and Grafana) pre-installed. Do the following to provision an ephemeral deluxe cluster: 1 2 3 4 5 6 7 8 9 10 11 $ cat /tmp/eks-deluxe.json { \"id\" : \"\" , \"name\" : \"mh9-deluxe\" , \"numworkers\" : 2 , \"kubeversion\" : \"1.13\" , \"timeout\" : 1440 , \"ttl\" : 1440 , \"owner\" : \"hausenbl+notif@amazon.com\" , \"created\" : \"\" } And then: 1 2 $ EKSPHEMERAL_EKSCTL_IMG = deluxe eksp create /tmp/eks-deluxe.json ... This takes some 15 min and after that, to access the Kubernetes dashboard we need to 1. proxy the UI locally, and 2. sort out the access control bits. First, launch the proxy to forward traffic to you local environment: 1 $ kubectl proxy Next, let's sort out the access control bits: First, create a service account eks-admin : 1 $ kubectl -n kube-system create sa eks-admin ... and give it cluster admin rights ... 1 2 3 $ kubectl create clusterrolebinding eks-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = kube-system:eks-admin ... and finally, get the token for logging into the dashboard: 1 2 3 $ kubectl -n kube-system describe secret \\ $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) | grep ^token token: e****************************************************************************************************************g Finally, got to the dashboard and use the value of token from the last command to log into the dashboard: Once you've completed the login, select the \"Namespace\" tab, left-hand side to check if all is as it should be, compare with the following screen shot: To access the ArgoCD UI , do: 1 $ kubectl port-forward svc/argocd-server -n argocd 8080 :443 And now execute: 1 2 $ kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd-server-7c7c748648-nlrwq Finally, open localhost:8080 and log in with username admin and password using the pod name from the previous step, in my case argocd-server-7c7c748648-nlrwq : Once you've completed the login, you are ready to configure your first app deployment, GitOps style: That's it. Now let's see how we can introspect and manipulate EKSphemeral clusters.","title":"Advanced cluster creation"},{"location":"cli/#list-clusters","text":"Next, let's check what clusters are managed by EKSphemeral: 1 2 3 $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 45 min 42 min hausenbl+notif@amazon.com Here, we get an tabular rendering of the clusters. We can use a cluster ID as follows to look up the spec of a particular cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ eksp list e90379cf-ee0a-49c7-8f82-1660760d6bb5 ID: e90379cf-ee0a-49c7-8f82-1660760d6bb5 Name: mh9-eksp Kubernetes: v1.12 Worker nodes: 2 Timeout: 45 min TTL: 38 min Owner: hausenbl+notif@amazon.com Details: Status: ACTIVE Endpoint: https://A377918A0CA6D8BE793FF8BEC88964FE.sk1.us-east-2.eks.amazonaws.com Platform version: eks.2 VPC config: private access: false, public access: true IAM role: arn:aws:iam::661776721573:role/eksctl-mh9-eksp-cluster-ServiceRole-1HT8OAOGNNY2Y","title":"List clusters"},{"location":"cli/#prolong-cluster-lifetime","text":"When you get a notification that one of your clusters is about to shut down or really at any time before it shuts down, you can prolong the cluster lifetime using the eksp prolong command. Let's say we want to keep the cluster with the ID e90379cf-ee0a-49c7-8f82-1660760d6bb5 around for 13 min longer. Here's what you would do: 1 2 3 4 $ eksp prolong e90379cf-ee0a-49c7-8f82-1660760d6bb5 13 Trying to set the TTL of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 to 13 minutes, starting now Successfully prolonged the lifetime of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 for 13 minutes. Now let's check: 1 2 3 $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 13 min 13 min hausenbl+notif@amazon.com Note The prolong command updates the timeout field of your cluster spec, that is, the cluster TTL is counted from the moment you issue the prolong command, taking the remaining cluster runtime into account.","title":"Prolong cluster lifetime"},{"location":"cli/#uninstall","text":"To uninstall EKSphemeral, use the following command. This will remove the control plane elements, that is, delete the Lambda functions and remove all cluster specs from the EKSPHEMERAL_CLUSTERMETA_BUCKET S3 bucket: 1 2 3 4 5 6 7 8 9 10 11 $ eksp uninstall Trying to uninstall EKSphemeral ... Taking down the EKSphemeral control plane, this might take a few minutes ... aws s3 rm s3://eks-cluster-meta --recursive aws cloudformation delete-stack --stack-name eksp Tear-down will complete within some 5 min. You can check the status manually, if you like, using 'make status' in the svc/ directory. Once you see a message saying something like 'Stack with id eksp does not exist' you know for sure it ' s gone : ) Thanks for using EKSphemeral and hope to see ya soon ; ) Warning The service code bucket and the cluster metadata bucket are still around after the uninstall command has completed. You can either manually delete them or keep them around, to reuse them later. This concludes the CLI walkthrough.","title":"Uninstall"},{"location":"dev/","text":"Development and testing \u00b6 If you want to play around with EKSphemeral, follow these steps. In order to build the service, clone this repo, and make sure you've got the following available, locally: The jq tool The aws CLI The SAM CLI The Fargate CLI Docker Also, you will need access to the following services, and their implicit dependencies, such as EC2 in case of EKS: AWS Lambda, AWS Fargate, Amazon EKS. The control plane \u00b6 The EKSphemeral control plane is implemented in AWS Lambda and S3, see also the architecture for details. In order for the local simulation, part of SAM, to work, you need to have Docker running. Note: Local testing the API is at time of writing not possible since CORS is locally not supported , yet. In the svc/ directory, do the following: 1 2 3 4 5 6 7 # 1. run emulation of Lambda and API Gateway locally (via Docker): $ sam local start-api # 2. update Go source code: add functionality, fix bugs # 3. create binaries, automagically synced into the local SAM runtime: $ make build If you change anything in the SAM/CF template file then you need to re-start the local API emulation. The EKSphemeral control plane has the following API: List the launched clusters via an HTTP GET to $BASEURL/status Check status of a specific cluster via an HTTP GET to $BASEURL/status/$CLUSTERID Create a cluster via an HTTP POST to $BASEURL/create with following parameters (all optional): numworkers ... number of worker nodes, defaults to 1 kubeversion ... Kubernetes version to use, defaults to 1.12 timeout ... timeout in minutes, after which the cluster is destroyed, defaults to 20 (and 5 minutes before that you get a warning mail) owner ... the email address of the owner Auto-destruction of a cluster after the set timeout (triggered by CloudWatch events, no HTTP endpoint) Once deployed, you can find out where the API runs via: 1 $ EKSPHEMERAL_URL = $( aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r ) The data plane \u00b6 The EKSphemeral data plane consists of eksctl running in AWS Fargate, see also the architecture for details. You can manually kick off the EKS cluster provisioning as described in the following. Note Optionally, you can build a custom container image using your own registry coordinates and customize what's in the eksctl image used to provision the EKS cluster via a Fargate task. First, set the security group to use: 1 $ export EKSPHEMERAL_SG = XXXX Note that if you don't know which default security group(s) you have available, you can use the following command to list them: 1 $ aws ec2 describe-security-groups | jq '.SecurityGroups[] | select (.GroupName == \"default\") | .GroupId' Also, you could create a dedicated security group for the data plane: 1 $ default_vpc = $( aws ec2 describe-vpcs --filters \"Name=isDefault, Values=true\" | jq .Vpcs [ 0 ] .VpcId -r ) And: 1 $ aws ec2 create-security-group --group-name eksphemeral-sg --description \"The security group the EKSphemeral data plane uses\" --vpc-id $default_vpc And: 1 $ aws ec2 authorize-security-group-ingress --group-name eksphemeral-sg --protocol all --port all Warning That the last command, aws ec2 authorize-security-group-ingress apparently doesn't work, unsure but based on my research it's an AWS CLI bug. Now you can use AWS Fargate through the Fargate CLI to provision the cluster, using your local AWS credentials, for example like so: 1 2 3 4 5 6 7 8 9 10 $ fargate task run eksctl \\ --image quay.io/mhausenblas/eksctl:base \\ --region us-east-2 \\ --env AWS_ACCESS_KEY_ID = $( aws configure get aws_access_key_id ) \\ --env AWS_SECRET_ACCESS_KEY = $( aws configure get aws_secret_access_key ) \\ --env AWS_DEFAULT_REGION = $( aws configure get region ) \\ --env CLUSTER_NAME = test \\ --env NUM_WORKERS = 3 \\ --env KUBERNETES_VERSION = 1 .12 \\ --security-group-id $EKSPHEMERAL_SG This should take something like 10 to 15 minutes to finish. Tip Keep an eye on the AWS console for the resources and logs. The UI \u00b6 If you want to change or extend the UI (HTML, JS, CSS) or the UI proxy you're welcome to do so. Warning Please make sure you're in the ui/ directory for the following steps. First, export the EKSPHEMERAL_URL env variable like so: 1 $ export EKSPHEMERAL_URL = $( aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r ) Now you can build the UI container image like so: 1 $ make build Verify that the image has been built and is available, locally: 1 $ make verify Now you can launch it: 1 2 3 4 5 6 7 8 9 10 11 12 $ make run docker run --name ekspui \\ --rm \\ --detach \\ --publish 8080 :8080 \\ --env EKSPHEMERAL_HOME = /eksp \\ --env AWS_ACCESS_KEY_ID = XXXX \\ --env AWS_SECRET_ACCESS_KEY = XXXX \\ --env AWS_DEFAULT_REGION = us-east-2 \\ --env EKSPHEMERAL_URL = https://nswn7lkjbk.execute-api.us-east-2.amazonaws.com/Prod \\ quay.io/mhausenblas/eksp-ui:0.2 79a352a4b0259e0b9731d5f3cfb942f185013ac51d14c4d4710eb7cfe1c534b2 Keep an eye on the logs of the UI proxy: 1 2 3 $ docker logs --follow ekspui 2019 /06/21 10 :06:58 EKSPhemeral UI up and running on http://localhost:8080/ ... When you're done, tear down the UI proxy: 1 2 3 $ make stop docker kill ekspui ekspui","title":"Develop"},{"location":"dev/#development-and-testing","text":"If you want to play around with EKSphemeral, follow these steps. In order to build the service, clone this repo, and make sure you've got the following available, locally: The jq tool The aws CLI The SAM CLI The Fargate CLI Docker Also, you will need access to the following services, and their implicit dependencies, such as EC2 in case of EKS: AWS Lambda, AWS Fargate, Amazon EKS.","title":"Development and testing"},{"location":"dev/#the-control-plane","text":"The EKSphemeral control plane is implemented in AWS Lambda and S3, see also the architecture for details. In order for the local simulation, part of SAM, to work, you need to have Docker running. Note: Local testing the API is at time of writing not possible since CORS is locally not supported , yet. In the svc/ directory, do the following: 1 2 3 4 5 6 7 # 1. run emulation of Lambda and API Gateway locally (via Docker): $ sam local start-api # 2. update Go source code: add functionality, fix bugs # 3. create binaries, automagically synced into the local SAM runtime: $ make build If you change anything in the SAM/CF template file then you need to re-start the local API emulation. The EKSphemeral control plane has the following API: List the launched clusters via an HTTP GET to $BASEURL/status Check status of a specific cluster via an HTTP GET to $BASEURL/status/$CLUSTERID Create a cluster via an HTTP POST to $BASEURL/create with following parameters (all optional): numworkers ... number of worker nodes, defaults to 1 kubeversion ... Kubernetes version to use, defaults to 1.12 timeout ... timeout in minutes, after which the cluster is destroyed, defaults to 20 (and 5 minutes before that you get a warning mail) owner ... the email address of the owner Auto-destruction of a cluster after the set timeout (triggered by CloudWatch events, no HTTP endpoint) Once deployed, you can find out where the API runs via: 1 $ EKSPHEMERAL_URL = $( aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r )","title":"The control plane"},{"location":"dev/#the-data-plane","text":"The EKSphemeral data plane consists of eksctl running in AWS Fargate, see also the architecture for details. You can manually kick off the EKS cluster provisioning as described in the following. Note Optionally, you can build a custom container image using your own registry coordinates and customize what's in the eksctl image used to provision the EKS cluster via a Fargate task. First, set the security group to use: 1 $ export EKSPHEMERAL_SG = XXXX Note that if you don't know which default security group(s) you have available, you can use the following command to list them: 1 $ aws ec2 describe-security-groups | jq '.SecurityGroups[] | select (.GroupName == \"default\") | .GroupId' Also, you could create a dedicated security group for the data plane: 1 $ default_vpc = $( aws ec2 describe-vpcs --filters \"Name=isDefault, Values=true\" | jq .Vpcs [ 0 ] .VpcId -r ) And: 1 $ aws ec2 create-security-group --group-name eksphemeral-sg --description \"The security group the EKSphemeral data plane uses\" --vpc-id $default_vpc And: 1 $ aws ec2 authorize-security-group-ingress --group-name eksphemeral-sg --protocol all --port all Warning That the last command, aws ec2 authorize-security-group-ingress apparently doesn't work, unsure but based on my research it's an AWS CLI bug. Now you can use AWS Fargate through the Fargate CLI to provision the cluster, using your local AWS credentials, for example like so: 1 2 3 4 5 6 7 8 9 10 $ fargate task run eksctl \\ --image quay.io/mhausenblas/eksctl:base \\ --region us-east-2 \\ --env AWS_ACCESS_KEY_ID = $( aws configure get aws_access_key_id ) \\ --env AWS_SECRET_ACCESS_KEY = $( aws configure get aws_secret_access_key ) \\ --env AWS_DEFAULT_REGION = $( aws configure get region ) \\ --env CLUSTER_NAME = test \\ --env NUM_WORKERS = 3 \\ --env KUBERNETES_VERSION = 1 .12 \\ --security-group-id $EKSPHEMERAL_SG This should take something like 10 to 15 minutes to finish. Tip Keep an eye on the AWS console for the resources and logs.","title":"The data plane"},{"location":"dev/#the-ui","text":"If you want to change or extend the UI (HTML, JS, CSS) or the UI proxy you're welcome to do so. Warning Please make sure you're in the ui/ directory for the following steps. First, export the EKSPHEMERAL_URL env variable like so: 1 $ export EKSPHEMERAL_URL = $( aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r ) Now you can build the UI container image like so: 1 $ make build Verify that the image has been built and is available, locally: 1 $ make verify Now you can launch it: 1 2 3 4 5 6 7 8 9 10 11 12 $ make run docker run --name ekspui \\ --rm \\ --detach \\ --publish 8080 :8080 \\ --env EKSPHEMERAL_HOME = /eksp \\ --env AWS_ACCESS_KEY_ID = XXXX \\ --env AWS_SECRET_ACCESS_KEY = XXXX \\ --env AWS_DEFAULT_REGION = us-east-2 \\ --env EKSPHEMERAL_URL = https://nswn7lkjbk.execute-api.us-east-2.amazonaws.com/Prod \\ quay.io/mhausenblas/eksp-ui:0.2 79a352a4b0259e0b9731d5f3cfb942f185013ac51d14c4d4710eb7cfe1c534b2 Keep an eye on the logs of the UI proxy: 1 2 3 $ docker logs --follow ekspui 2019 /06/21 10 :06:58 EKSPhemeral UI up and running on http://localhost:8080/ ... When you're done, tear down the UI proxy: 1 2 3 $ make stop docker kill ekspui ekspui","title":"The UI"},{"location":"faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Use Cases \u00b6 Question What is EKSphemeral for? Answer For development, demo, and test environments. Costs \u00b6 Question What are the cost savings? Answer Depends on how long the EKS cluster runs. The EKSphemeral control plane components virtually do not cost anything (few cents per month at maximum). CLI \u00b6 Question Does the EKSphemeral CLI require Docker? Answer No. Only for the UI you need to have Docker running, locally. Customize it \u00b6 Question Can I define my own images? The base image is to little for my needs and I don't like what you put together in the deluxe image. Answer Sure! Check out the development and testing docs. Misc \u00b6 Question How is EKSphemeral pronounced? Answer Glad you asked, I thought that would never happen: x-phemeral.","title":"FAQ"},{"location":"faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/#use-cases","text":"Question What is EKSphemeral for? Answer For development, demo, and test environments.","title":"Use Cases"},{"location":"faq/#costs","text":"Question What are the cost savings? Answer Depends on how long the EKS cluster runs. The EKSphemeral control plane components virtually do not cost anything (few cents per month at maximum).","title":"Costs"},{"location":"faq/#cli","text":"Question Does the EKSphemeral CLI require Docker? Answer No. Only for the UI you need to have Docker running, locally.","title":"CLI"},{"location":"faq/#customize-it","text":"Question Can I define my own images? The base image is to little for my needs and I don't like what you put together in the deluxe image. Answer Sure! Check out the development and testing docs.","title":"Customize it"},{"location":"faq/#misc","text":"Question How is EKSphemeral pronounced? Answer Glad you asked, I thought that would never happen: x-phemeral.","title":"Misc"},{"location":"ui/","text":"The EKSphemeral web UI \u00b6 Note In order to use the EKSphemeral web UI locally, you need to have Docker installed and running. To launch the UI, do the following: 1 2 3 $ ./launch-ui.sh 2019 /06/21 10 :15:41 EKSPhemeral UI up and running on http://localhost:8080/ ^C Optionally, you can tear down the UI proxy manually like so (will be done automatically on launch): 1 2 3 $ ./stop-ui.sh docker kill ekspui ekspui Now, head over to http://localhost:8080 and you should see something like the following. First, on start up: When you create a new cluster: When you want to configure your local environment to use the cluster: When you want to prolong the life time of a cluster:","title":"Web UI"},{"location":"ui/#the-eksphemeral-web-ui","text":"Note In order to use the EKSphemeral web UI locally, you need to have Docker installed and running. To launch the UI, do the following: 1 2 3 $ ./launch-ui.sh 2019 /06/21 10 :15:41 EKSPhemeral UI up and running on http://localhost:8080/ ^C Optionally, you can tear down the UI proxy manually like so (will be done automatically on launch): 1 2 3 $ ./stop-ui.sh docker kill ekspui ekspui Now, head over to http://localhost:8080 and you should see something like the following. First, on start up: When you create a new cluster: When you want to configure your local environment to use the cluster: When you want to prolong the life time of a cluster:","title":"The EKSphemeral web UI"}]}