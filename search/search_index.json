{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The EKS Ephemeral Cluster Manager # Note This is a service for development and test environments. It is not an official AWS offering, use at your own risk. Managing Amazon EKS clusters for development and test environments manually is boring. You have to wait until they're provisioned and then have to remember to tear them down again, in order to minimize costs. How about automating these steps? Meet EKSphemeral, the simple manager for ephemeral EKS clusters, allowing you to launch EKS clusters that auto-tear down after some time, and you can also prolong their lifetime if you want to continue to use them. You can either use the EKSphemeral UI : Or the EKSphemeral CLI : Install # In order to use EKSphemeral you need to have jq ( install ) and the aws CLI ( install ) installed. All other dependencies, such as the Fargate CLI will be installed automatically, if not present on the system. Warning Make sure to set the respective environment variables as shown here before you proceed. Most install issue come from not all environment variables set. The following environment variables need to be set so that the install process knows where the dependencies are and which S3 bucket to use for the control plane ( EKSPHEMERAL_SVC_BUCKET ) and where to put the cluster metadata ( EKSPHEMERAL_CLUSTERMETA_BUCKET ), for example: $ export EKSPHEMERAL_HOME=~/eksp $ export EKSPHEMERAL_SVC_BUCKET=eks-svc $ export EKSPHEMERAL_CLUSTERMETA_BUCKET=eks-cluster-meta Optionally, in order to receive email notifications about cluster creation and destruction, you need to set the EKSPHEMERAL_EMAIL_FROM environment variable, for example: $ export EKSPHEMERAL_EMAIL_FROM=hausenbl+eksphemeral@amazon.com Note In addition to setting the EKSPHEMERAL_EMAIL_FROM environment variable, you MUST verify both the source email, that is, the address you provide in EKSPHEMERAL_EMAIL_FROM as well as the target email address (in the owner field of the cluster spec, see below for details) in the EU (Ireland) eu-west-1 region. We're now in the position to install EKSphemeral with a single command, here shown for an install below your home directory: $ curl -sL http://get.eksphemeral.info/install.sh | sudo --preserve-env bash This process can take several minutes. After this, EKSphemeral is installed in your AWS environment as well as the CLI is locally available. Learn more about what exactly is created and running as part of the install process by perusing the EKSphemeral architecture . Use # You can create, inspect, and prolong the lifetime of a cluster with the CLI or, if you prefer a visual interface check out the local EKSphemeral UI proxy, which requires Docker.","title":"Home"},{"location":"#the-eks-ephemeral-cluster-manager","text":"Note This is a service for development and test environments. It is not an official AWS offering, use at your own risk. Managing Amazon EKS clusters for development and test environments manually is boring. You have to wait until they're provisioned and then have to remember to tear them down again, in order to minimize costs. How about automating these steps? Meet EKSphemeral, the simple manager for ephemeral EKS clusters, allowing you to launch EKS clusters that auto-tear down after some time, and you can also prolong their lifetime if you want to continue to use them. You can either use the EKSphemeral UI : Or the EKSphemeral CLI :","title":"The EKS Ephemeral Cluster Manager"},{"location":"#install","text":"In order to use EKSphemeral you need to have jq ( install ) and the aws CLI ( install ) installed. All other dependencies, such as the Fargate CLI will be installed automatically, if not present on the system. Warning Make sure to set the respective environment variables as shown here before you proceed. Most install issue come from not all environment variables set. The following environment variables need to be set so that the install process knows where the dependencies are and which S3 bucket to use for the control plane ( EKSPHEMERAL_SVC_BUCKET ) and where to put the cluster metadata ( EKSPHEMERAL_CLUSTERMETA_BUCKET ), for example: $ export EKSPHEMERAL_HOME=~/eksp $ export EKSPHEMERAL_SVC_BUCKET=eks-svc $ export EKSPHEMERAL_CLUSTERMETA_BUCKET=eks-cluster-meta Optionally, in order to receive email notifications about cluster creation and destruction, you need to set the EKSPHEMERAL_EMAIL_FROM environment variable, for example: $ export EKSPHEMERAL_EMAIL_FROM=hausenbl+eksphemeral@amazon.com Note In addition to setting the EKSPHEMERAL_EMAIL_FROM environment variable, you MUST verify both the source email, that is, the address you provide in EKSPHEMERAL_EMAIL_FROM as well as the target email address (in the owner field of the cluster spec, see below for details) in the EU (Ireland) eu-west-1 region. We're now in the position to install EKSphemeral with a single command, here shown for an install below your home directory: $ curl -sL http://get.eksphemeral.info/install.sh | sudo --preserve-env bash This process can take several minutes. After this, EKSphemeral is installed in your AWS environment as well as the CLI is locally available. Learn more about what exactly is created and running as part of the install process by perusing the EKSphemeral architecture .","title":"Install"},{"location":"#use","text":"You can create, inspect, and prolong the lifetime of a cluster with the CLI or, if you prefer a visual interface check out the local EKSphemeral UI proxy, which requires Docker.","title":"Use"},{"location":"arch/","text":"Architecture # EKSphemeral has a control plane implemented in an AWS Lambda/Amazon S3 combo, and as its data plane it is using eksctl running in AWS Fargate. The architecture looks as follows: With eksp install you provisions EKSphemeral's control plane (Lambda+S3). Whenever you want to provision a throwaway EKS cluster, use eksp create . It will do two things: Provision the cluster using eksctl running in Fargate, and when that is completed, Create an cluster spec entry in S3, via the /create endpoint of EKSphemeral's HTTP API. Every five minutes, a CloudWatch event triggers the execution of another Lambda function called DestroyClusterFunc , which notifies the owners of clusters that are about to expire (send an email up to 5 minutes before the cluster is destroyed), and when the time comes, it tears the cluster down. Once the EKS cluster is provisioned and the Kubernetes context is configured you can use your cluster. You can use eksp list (via the /status endpoint) at any time to list managed clusters. If you want to keep your cluster around longer, use eksp prolong (via the /prolong endpoint) to extend its lifetime. Last but not least, if you want to get rid of EKSphemeral, use the eksp uninstall , removing all cluster specs in the S3 bucket and deleting all Lambda functions.","title":"Architecture"},{"location":"arch/#architecture","text":"EKSphemeral has a control plane implemented in an AWS Lambda/Amazon S3 combo, and as its data plane it is using eksctl running in AWS Fargate. The architecture looks as follows: With eksp install you provisions EKSphemeral's control plane (Lambda+S3). Whenever you want to provision a throwaway EKS cluster, use eksp create . It will do two things: Provision the cluster using eksctl running in Fargate, and when that is completed, Create an cluster spec entry in S3, via the /create endpoint of EKSphemeral's HTTP API. Every five minutes, a CloudWatch event triggers the execution of another Lambda function called DestroyClusterFunc , which notifies the owners of clusters that are about to expire (send an email up to 5 minutes before the cluster is destroyed), and when the time comes, it tears the cluster down. Once the EKS cluster is provisioned and the Kubernetes context is configured you can use your cluster. You can use eksp list (via the /status endpoint) at any time to list managed clusters. If you want to keep your cluster around longer, use eksp prolong (via the /prolong endpoint) to extend its lifetime. Last but not least, if you want to get rid of EKSphemeral, use the eksp uninstall , removing all cluster specs in the S3 bucket and deleting all Lambda functions.","title":"Architecture"},{"location":"cli/","text":"The EKSphemeral CLI # Note Currently, the CLI binaries are available for both macOS and Linux platforms. You can create, inspect, and prolong the lifetime of a cluster with the CLI as shown in the following. Manual install # Note You usually don't need to install the CLI manually, it should have been set up with the overall install. However, in cases where you want to access EKSphemeral from a machine other than the one you set it up originally or the CLI has been removed by someone or something, follow the steps here. To manually install the binary CLI, for example on macOS, do: $ curl -sL https://github.com/mhausenblas/eksphemeral/releases/latest/download/eksp-macos -o eksp $ chmod +x eksp $ sudo mv ./eksp /usr/local/bin Now, let's check if there are already clusters are managed by EKSphemeral: $ eksp list No clusters found Since we just installed EKSphemeral, there are no clusters, yet. Let's change that. Create clusters # Let's create a cluster named mh9-eksp , with three worker nodes, using Kubernetes version 1.12, with a 150 min timeout. First, create a file cluster-spec.json with the following content: { \"id\": \"\", \"name\": \"mh9-eksp\", \"numworkers\": 3, \"kubeversion\": \"1.21\", \"timeout\": 150, \"ttl\": 150, \"owner\": \"hausenbl+notif@amazon.com\", \"created\": \"\" } Now you can use the create command like so: $ eksp create luster-spec.json Trying to create a new ephemeral cluster ... ... using cluster spec cluster-spec.json Seems you have set 'us-east-2' as the target region, using this for all following operations I will now provision the EKS cluster mh9-eksp using AWS Fargate: [i] Running task eksctl Waiting for EKS cluster provisioning to complete. Allow some 15 min to complete, checking status every minute: ......... Successfully created data plane for cluster mh9-eksp using AWS Fargate and now moving on to the control plane in AWS Lambda and S3 ... Successfully created control plane entry for cluster mh9-eksp via AWS Lambda and Amazon S3 ... Now moving on to configure kubectl to point to your EKS cluster: Updated context arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp in /Users/hausenbl/.kube/config Your EKS cluster is now set up and configured: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp Note that it still can take up to 5 min until the worker nodes are available, check with the following command until you don't see the 'No resources found.' message anymore: kubectl get nodes Note that if no cluster spec is provided, a default cluster spec will be used along with first security group of the default VPC. Once the cluster is ready and you've verified your email addresses you should get a notification that looks something like the following: The same is true at least five minutes before the cluster shuts down. List clusters # Next, let's check what clusters are managed by EKSphemeral: $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 45 min 42 min hausenbl+notif@amazon.com Here, we get an array of cluster IDs back. We can use such a cluster ID as follows to look up the spec of a particular cluster: $ eksp list e90379cf-ee0a-49c7-8f82-1660760d6bb5 ID: e90379cf-ee0a-49c7-8f82-1660760d6bb5 Name: mh9-eksp Kubernetes: v1.12 Worker nodes: 2 Timeout: 45 min TTL: 38 min Owner: hausenbl+notif@amazon.com Details: Status: ACTIVE Endpoint: https://A377918A0CA6D8BE793FF8BEC88964FE.sk1.us-east-2.eks.amazonaws.com Platform version: eks.2 VPC config: private access: false, public access: true IAM role: arn:aws:iam::661776721573:role/eksctl-mh9-eksp-cluster-ServiceRole-1HT8OAOGNNY2Y Prolong cluster lifetime # When you get a notification that one of your clusters is about to shut down or really at any time before it shuts down, you can prolong the cluster lifetime using the eksp prolong command. Let's say we want to keep the cluster with the ID e90379cf-ee0a-49c7-8f82-1660760d6bb5 around for 13 min longer. Here's what you would do: $ eksp prolong e90379cf-ee0a-49c7-8f82-1660760d6bb5 13 Trying to set the TTL of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 to 13 minutes, starting now Successfully prolonged the lifetime of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 for 13 minutes. $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 13 min 13 min hausenbl+notif@amazon.com Note that the prolong command updates the timeout field of your cluster spec, that is, the cluster TTL is counted from the moment you issue the prolong command, taking the remaining cluster runtime into account. Uninstall # To uninstall EKSphemeral, use the following command. This will remove the control plane elements, that is, delete the Lambda functions and remove all cluster specs from the EKSPHEMERAL_CLUSTERMETA_BUCKET S3 bucket: $ eksp uninstall Trying to uninstall EKSphemeral ... Taking down the EKSphemeral control plane, this might take a few minutes ... aws s3 rm s3://eks-cluster-meta --recursive aws cloudformation delete-stack --stack-name eksp Tear-down will complete within some 5 min. You can check the status manually, if you like, using 'make status' in the svc/ directory. Once you see a message saying something like 'Stack with id eksp does not exist' you know for sure it's gone :) Thanks for using EKSphemeral and hope to see ya soon ;) Note that the service code bucket and the cluster metadata bucket are still around after this. You can either manually delete them or keep them around, to reuse them later.","title":"CLI"},{"location":"cli/#the-eksphemeral-cli","text":"Note Currently, the CLI binaries are available for both macOS and Linux platforms. You can create, inspect, and prolong the lifetime of a cluster with the CLI as shown in the following.","title":"The EKSphemeral CLI"},{"location":"cli/#manual-install","text":"Note You usually don't need to install the CLI manually, it should have been set up with the overall install. However, in cases where you want to access EKSphemeral from a machine other than the one you set it up originally or the CLI has been removed by someone or something, follow the steps here. To manually install the binary CLI, for example on macOS, do: $ curl -sL https://github.com/mhausenblas/eksphemeral/releases/latest/download/eksp-macos -o eksp $ chmod +x eksp $ sudo mv ./eksp /usr/local/bin Now, let's check if there are already clusters are managed by EKSphemeral: $ eksp list No clusters found Since we just installed EKSphemeral, there are no clusters, yet. Let's change that.","title":"Manual install"},{"location":"cli/#create-clusters","text":"Let's create a cluster named mh9-eksp , with three worker nodes, using Kubernetes version 1.12, with a 150 min timeout. First, create a file cluster-spec.json with the following content: { \"id\": \"\", \"name\": \"mh9-eksp\", \"numworkers\": 3, \"kubeversion\": \"1.21\", \"timeout\": 150, \"ttl\": 150, \"owner\": \"hausenbl+notif@amazon.com\", \"created\": \"\" } Now you can use the create command like so: $ eksp create luster-spec.json Trying to create a new ephemeral cluster ... ... using cluster spec cluster-spec.json Seems you have set 'us-east-2' as the target region, using this for all following operations I will now provision the EKS cluster mh9-eksp using AWS Fargate: [i] Running task eksctl Waiting for EKS cluster provisioning to complete. Allow some 15 min to complete, checking status every minute: ......... Successfully created data plane for cluster mh9-eksp using AWS Fargate and now moving on to the control plane in AWS Lambda and S3 ... Successfully created control plane entry for cluster mh9-eksp via AWS Lambda and Amazon S3 ... Now moving on to configure kubectl to point to your EKS cluster: Updated context arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp in /Users/hausenbl/.kube/config Your EKS cluster is now set up and configured: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp arn:aws:eks:us-east-2:661776721573:cluster/mh9-eksp Note that it still can take up to 5 min until the worker nodes are available, check with the following command until you don't see the 'No resources found.' message anymore: kubectl get nodes Note that if no cluster spec is provided, a default cluster spec will be used along with first security group of the default VPC. Once the cluster is ready and you've verified your email addresses you should get a notification that looks something like the following: The same is true at least five minutes before the cluster shuts down.","title":"Create clusters"},{"location":"cli/#list-clusters","text":"Next, let's check what clusters are managed by EKSphemeral: $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 45 min 42 min hausenbl+notif@amazon.com Here, we get an array of cluster IDs back. We can use such a cluster ID as follows to look up the spec of a particular cluster: $ eksp list e90379cf-ee0a-49c7-8f82-1660760d6bb5 ID: e90379cf-ee0a-49c7-8f82-1660760d6bb5 Name: mh9-eksp Kubernetes: v1.12 Worker nodes: 2 Timeout: 45 min TTL: 38 min Owner: hausenbl+notif@amazon.com Details: Status: ACTIVE Endpoint: https://A377918A0CA6D8BE793FF8BEC88964FE.sk1.us-east-2.eks.amazonaws.com Platform version: eks.2 VPC config: private access: false, public access: true IAM role: arn:aws:iam::661776721573:role/eksctl-mh9-eksp-cluster-ServiceRole-1HT8OAOGNNY2Y","title":"List clusters"},{"location":"cli/#prolong-cluster-lifetime","text":"When you get a notification that one of your clusters is about to shut down or really at any time before it shuts down, you can prolong the cluster lifetime using the eksp prolong command. Let's say we want to keep the cluster with the ID e90379cf-ee0a-49c7-8f82-1660760d6bb5 around for 13 min longer. Here's what you would do: $ eksp prolong e90379cf-ee0a-49c7-8f82-1660760d6bb5 13 Trying to set the TTL of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 to 13 minutes, starting now Successfully prolonged the lifetime of cluster e90379cf-ee0a-49c7-8f82-1660760d6bb5 for 13 minutes. $ eksp list NAME ID KUBERNETES NUM WORKERS TIMEOUT TTL OWNER mh9-eksp e90379cf-ee0a-49c7-8f82-1660760d6bb5 v1.12 2 13 min 13 min hausenbl+notif@amazon.com Note that the prolong command updates the timeout field of your cluster spec, that is, the cluster TTL is counted from the moment you issue the prolong command, taking the remaining cluster runtime into account.","title":"Prolong cluster lifetime"},{"location":"cli/#uninstall","text":"To uninstall EKSphemeral, use the following command. This will remove the control plane elements, that is, delete the Lambda functions and remove all cluster specs from the EKSPHEMERAL_CLUSTERMETA_BUCKET S3 bucket: $ eksp uninstall Trying to uninstall EKSphemeral ... Taking down the EKSphemeral control plane, this might take a few minutes ... aws s3 rm s3://eks-cluster-meta --recursive aws cloudformation delete-stack --stack-name eksp Tear-down will complete within some 5 min. You can check the status manually, if you like, using 'make status' in the svc/ directory. Once you see a message saying something like 'Stack with id eksp does not exist' you know for sure it's gone :) Thanks for using EKSphemeral and hope to see ya soon ;) Note that the service code bucket and the cluster metadata bucket are still around after this. You can either manually delete them or keep them around, to reuse them later.","title":"Uninstall"},{"location":"dev/","text":"Development and testing # If you want to play around with EKSphemeral, follow these steps. In order to build the service, clone this repo, and make sure you've got the following available, locally: The jq tool The aws CLI The SAM CLI The Fargate CLI Docker Also, you will need access to the following services, and their implicit dependencies, such as EC2 in case of EKS: AWS Lambda, AWS Fargate, Amazon EKS. The control plane # The EKSphemeral control plane is implemented in AWS Lambda and S3, see also the architecture for details. In order for the local simulation, part of SAM, to work, you need to have Docker running. Note: Local testing the API is at time of writing not possible since CORS is locally not supported , yet. In the svc/ directory, do the following: # 1. run emulation of Lambda and API Gateway locally (via Docker): $ sam local start-api # 2. update Go source code: add functionality, fix bugs # 3. create binaries, automagically synced into the local SAM runtime: $ make build If you change anything in the SAM/CF template file then you need to re-start the local API emulation. The EKSphemeral control plane has the following API: List the launched clusters via an HTTP GET to $BASEURL/status Check status of a specific cluster via an HTTP GET to $BASEURL/status/$CLUSTERID Create a cluster via an HTTP POST to $BASEURL/create with following parameters (all optional): numworkers ... number of worker nodes, defaults to 1 kubeversion ... Kubernetes version to use, defaults to 1.12 timeout ... timeout in minutes, after which the cluster is destroyed, defaults to 20 (and 5 minutes before that you get a warning mail) owner ... the email address of the owner Auto-destruction of a cluster after the set timeout (triggered by CloudWatch events, no HTTP endpoint) Once deployed, you can find out where the API runs via: $ EKSPHEMERAL_URL=$(aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r) The data plane # The EKSphemeral data plane consists of eksctl running in AWS Fargate, see also the architecture for details. You can manually kick off the EKS cluster provisioning as described in the following. Note Optionally, you can build a custom container image using your own registry coordinates and customize what's in the eksctl image used to provision the EKS cluster via a Fargate task. First, set the security group to use: $ export EKSPHEMERAL_SG=XXXX Note that if you don't know which default security group(s) you have available, you can use the following command to list them: $ aws ec2 describe-security-groups | jq '.SecurityGroups[] | select (.GroupName == \"default\") | .GroupId' Also, you could create a dedicated security group for the data plane: $ default_vpc=$(aws ec2 describe-vpcs --filters \"Name=isDefault, Values=true\" | jq .Vpcs[0].VpcId -r) And: $ aws ec2 create-security-group --group-name eksphemeral-sg --description \"The security group the EKSphemeral data plane uses\" --vpc-id $default_vpc And: $ aws ec2 authorize-security-group-ingress --group-name eksphemeral-sg --protocol all --port all Warning That the last command, aws ec2 authorize-security-group-ingress apparently doesn't work, unsure but based on my research it's an AWS CLI bug. Now you can use AWS Fargate through the Fargate CLI to provision the cluster, using your local AWS credentials, for example like so: $ fargate task run eksctl \\ --image quay.io/mhausenblas/eksctl:base \\ --region us-east-2 \\ --env AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) \\ --env AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) \\ --env AWS_DEFAULT_REGION=$(aws configure get region) \\ --env CLUSTER_NAME=test \\ --env NUM_WORKERS=3 \\ --env KUBERNETES_VERSION=1.12 \\ --security-group-id $EKSPHEMERAL_SG This should take something like 10 to 15 minutes to finish. Tip Keep an eye on the AWS console for the resources and logs. The UI # If you want to change or extend the UI (HTML, JS, CSS) or the UI proxy you're welcome to do so. Warning Please make sure you're in the ui/ directory for the following steps. First, export the EKSPHEMERAL_URL env variable like so: $ export EKSPHEMERAL_URL=$(aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r) Now you can build the UI container image like so: $ make build GOOS=linux GOARCH=amd64 go build -o ./proxy . docker build --tag quay.io/mhausenblas/eksp-ui:0.2 . Sending build context to Docker daemon 10.86MB Step 1/17 : FROM amazonlinux:2018.03 ---> a89f4a191d4c Step 2/17 : LABEL maintainer=\"Michael Hausenblas <hausenbl@amazon.com>\" ---> Using cache ---> f57e0623e5d8 Step 3/17 : ARG AWS_ACCESS_KEY_ID ---> Using cache ---> 45fc8c05256c Step 4/17 : ARG AWS_SECRET_ACCESS_KEY ---> Using cache ---> 02a4bcc33f74 Step 5/17 : ARG AWS_DEFAULT_REGION ---> Using cache ---> 6d1562974f7c Step 6/17 : COPY install.sh . ---> Using cache ---> 5a6d45d855ce Step 7/17 : RUN yum install unzip jq git -y && yum clean all && curl -sL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && pip install awscli --upgrade && export EKSPHEMERAL_HOME=/eksp && chmod +x install.sh && ./install.sh ---> Using cache ---> dcb28e56940f Step 8/17 : COPY css/* /app/css/ ---> Using cache ---> f2aad01c40dc Step 9/17 : COPY img/* /app/img/ ---> Using cache ---> ae2750fca6cd Step 10/17 : COPY js/* /app/js/ ---> Using cache ---> 82fec12cf16a Step 11/17 : COPY *.html /app/ ---> Using cache ---> db13d828ff73 Step 12/17 : WORKDIR /app ---> Using cache ---> 304d80268fbe Step 13/17 : RUN chown -R 1001:1 /app ---> Using cache ---> 0c09d65e5358 Step 14/17 : USER 1001 ---> Using cache ---> 6ca8e15efb82 Step 15/17 : COPY proxy . ---> Using cache ---> 397672a796b3 Step 16/17 : EXPOSE 8080 ---> Using cache ---> 6508d74b4e39 Step 17/17 : CMD [\"/app/proxy\"] ---> Using cache ---> 2ad45f31e101 Successfully built 2ad45f31e101 Successfully tagged quay.io/mhausenblas/eksp-ui:0.2 Verify that the image has been built and is available, locally: $ make verify REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/mhausenblas/eksp-ui 0.2 2ad45f31e101 About an hour ago 449MB Now you can launch it: $ make run docker run --name ekspui \\ --rm \\ --detach \\ --publish 8080:8080 \\ --env EKSPHEMERAL_HOME=/eksp \\ --env AWS_ACCESS_KEY_ID=XXXX \\ --env AWS_SECRET_ACCESS_KEY=XXXX \\ --env AWS_DEFAULT_REGION=us-east-2 \\ --env EKSPHEMERAL_URL=https://nswn7lkjbk.execute-api.us-east-2.amazonaws.com/Prod \\ quay.io/mhausenblas/eksp-ui:0.2 79a352a4b0259e0b9731d5f3cfb942f185013ac51d14c4d4710eb7cfe1c534b2 Keep an eye on the logs of the UI proxy: $ docker logs --follow ekspui 2019/06/21 10:06:58 EKSPhemeral UI up and running on http://localhost:8080/ ... When you're done, tear down the UI proxy: $ make stop docker kill ekspui ekspui","title":"Develop"},{"location":"dev/#development-and-testing","text":"If you want to play around with EKSphemeral, follow these steps. In order to build the service, clone this repo, and make sure you've got the following available, locally: The jq tool The aws CLI The SAM CLI The Fargate CLI Docker Also, you will need access to the following services, and their implicit dependencies, such as EC2 in case of EKS: AWS Lambda, AWS Fargate, Amazon EKS.","title":"Development and testing"},{"location":"dev/#the-control-plane","text":"The EKSphemeral control plane is implemented in AWS Lambda and S3, see also the architecture for details. In order for the local simulation, part of SAM, to work, you need to have Docker running. Note: Local testing the API is at time of writing not possible since CORS is locally not supported , yet. In the svc/ directory, do the following: # 1. run emulation of Lambda and API Gateway locally (via Docker): $ sam local start-api # 2. update Go source code: add functionality, fix bugs # 3. create binaries, automagically synced into the local SAM runtime: $ make build If you change anything in the SAM/CF template file then you need to re-start the local API emulation. The EKSphemeral control plane has the following API: List the launched clusters via an HTTP GET to $BASEURL/status Check status of a specific cluster via an HTTP GET to $BASEURL/status/$CLUSTERID Create a cluster via an HTTP POST to $BASEURL/create with following parameters (all optional): numworkers ... number of worker nodes, defaults to 1 kubeversion ... Kubernetes version to use, defaults to 1.12 timeout ... timeout in minutes, after which the cluster is destroyed, defaults to 20 (and 5 minutes before that you get a warning mail) owner ... the email address of the owner Auto-destruction of a cluster after the set timeout (triggered by CloudWatch events, no HTTP endpoint) Once deployed, you can find out where the API runs via: $ EKSPHEMERAL_URL=$(aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r)","title":"The control plane"},{"location":"dev/#the-data-plane","text":"The EKSphemeral data plane consists of eksctl running in AWS Fargate, see also the architecture for details. You can manually kick off the EKS cluster provisioning as described in the following. Note Optionally, you can build a custom container image using your own registry coordinates and customize what's in the eksctl image used to provision the EKS cluster via a Fargate task. First, set the security group to use: $ export EKSPHEMERAL_SG=XXXX Note that if you don't know which default security group(s) you have available, you can use the following command to list them: $ aws ec2 describe-security-groups | jq '.SecurityGroups[] | select (.GroupName == \"default\") | .GroupId' Also, you could create a dedicated security group for the data plane: $ default_vpc=$(aws ec2 describe-vpcs --filters \"Name=isDefault, Values=true\" | jq .Vpcs[0].VpcId -r) And: $ aws ec2 create-security-group --group-name eksphemeral-sg --description \"The security group the EKSphemeral data plane uses\" --vpc-id $default_vpc And: $ aws ec2 authorize-security-group-ingress --group-name eksphemeral-sg --protocol all --port all Warning That the last command, aws ec2 authorize-security-group-ingress apparently doesn't work, unsure but based on my research it's an AWS CLI bug. Now you can use AWS Fargate through the Fargate CLI to provision the cluster, using your local AWS credentials, for example like so: $ fargate task run eksctl \\ --image quay.io/mhausenblas/eksctl:base \\ --region us-east-2 \\ --env AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) \\ --env AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) \\ --env AWS_DEFAULT_REGION=$(aws configure get region) \\ --env CLUSTER_NAME=test \\ --env NUM_WORKERS=3 \\ --env KUBERNETES_VERSION=1.12 \\ --security-group-id $EKSPHEMERAL_SG This should take something like 10 to 15 minutes to finish. Tip Keep an eye on the AWS console for the resources and logs.","title":"The data plane"},{"location":"dev/#the-ui","text":"If you want to change or extend the UI (HTML, JS, CSS) or the UI proxy you're welcome to do so. Warning Please make sure you're in the ui/ directory for the following steps. First, export the EKSPHEMERAL_URL env variable like so: $ export EKSPHEMERAL_URL=$(aws cloudformation describe-stacks --stack-name eksp | jq '.Stacks[].Outputs[] | select(.OutputKey==\"EKSphemeralAPIEndpoint\").OutputValue' -r) Now you can build the UI container image like so: $ make build GOOS=linux GOARCH=amd64 go build -o ./proxy . docker build --tag quay.io/mhausenblas/eksp-ui:0.2 . Sending build context to Docker daemon 10.86MB Step 1/17 : FROM amazonlinux:2018.03 ---> a89f4a191d4c Step 2/17 : LABEL maintainer=\"Michael Hausenblas <hausenbl@amazon.com>\" ---> Using cache ---> f57e0623e5d8 Step 3/17 : ARG AWS_ACCESS_KEY_ID ---> Using cache ---> 45fc8c05256c Step 4/17 : ARG AWS_SECRET_ACCESS_KEY ---> Using cache ---> 02a4bcc33f74 Step 5/17 : ARG AWS_DEFAULT_REGION ---> Using cache ---> 6d1562974f7c Step 6/17 : COPY install.sh . ---> Using cache ---> 5a6d45d855ce Step 7/17 : RUN yum install unzip jq git -y && yum clean all && curl -sL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && pip install awscli --upgrade && export EKSPHEMERAL_HOME=/eksp && chmod +x install.sh && ./install.sh ---> Using cache ---> dcb28e56940f Step 8/17 : COPY css/* /app/css/ ---> Using cache ---> f2aad01c40dc Step 9/17 : COPY img/* /app/img/ ---> Using cache ---> ae2750fca6cd Step 10/17 : COPY js/* /app/js/ ---> Using cache ---> 82fec12cf16a Step 11/17 : COPY *.html /app/ ---> Using cache ---> db13d828ff73 Step 12/17 : WORKDIR /app ---> Using cache ---> 304d80268fbe Step 13/17 : RUN chown -R 1001:1 /app ---> Using cache ---> 0c09d65e5358 Step 14/17 : USER 1001 ---> Using cache ---> 6ca8e15efb82 Step 15/17 : COPY proxy . ---> Using cache ---> 397672a796b3 Step 16/17 : EXPOSE 8080 ---> Using cache ---> 6508d74b4e39 Step 17/17 : CMD [\"/app/proxy\"] ---> Using cache ---> 2ad45f31e101 Successfully built 2ad45f31e101 Successfully tagged quay.io/mhausenblas/eksp-ui:0.2 Verify that the image has been built and is available, locally: $ make verify REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/mhausenblas/eksp-ui 0.2 2ad45f31e101 About an hour ago 449MB Now you can launch it: $ make run docker run --name ekspui \\ --rm \\ --detach \\ --publish 8080:8080 \\ --env EKSPHEMERAL_HOME=/eksp \\ --env AWS_ACCESS_KEY_ID=XXXX \\ --env AWS_SECRET_ACCESS_KEY=XXXX \\ --env AWS_DEFAULT_REGION=us-east-2 \\ --env EKSPHEMERAL_URL=https://nswn7lkjbk.execute-api.us-east-2.amazonaws.com/Prod \\ quay.io/mhausenblas/eksp-ui:0.2 79a352a4b0259e0b9731d5f3cfb942f185013ac51d14c4d4710eb7cfe1c534b2 Keep an eye on the logs of the UI proxy: $ docker logs --follow ekspui 2019/06/21 10:06:58 EKSPhemeral UI up and running on http://localhost:8080/ ... When you're done, tear down the UI proxy: $ make stop docker kill ekspui ekspui","title":"The UI"},{"location":"ui/","text":"The EKSphemeral web UI # Note In order to use the EKSphemeral web UI locally, you need to have Docker installed and running. To launch the UI, do the following: $ ./launch-ui.sh 2019/06/21 10:15:41 EKSPhemeral UI up and running on http://localhost:8080/ ^C Optionally, you can tear down the UI proxy manually like so (will be done automatically on launch): $ ./stop-ui.sh docker kill ekspui ekspui Now, head over to http://localhost:8080 and you should see something like the following. First, on start up: When you create a new cluster: When you want to configure your local environment to use the cluster: When you want to prolong the life time of a cluster:","title":"Web UI"},{"location":"ui/#the-eksphemeral-web-ui","text":"Note In order to use the EKSphemeral web UI locally, you need to have Docker installed and running. To launch the UI, do the following: $ ./launch-ui.sh 2019/06/21 10:15:41 EKSPhemeral UI up and running on http://localhost:8080/ ^C Optionally, you can tear down the UI proxy manually like so (will be done automatically on launch): $ ./stop-ui.sh docker kill ekspui ekspui Now, head over to http://localhost:8080 and you should see something like the following. First, on start up: When you create a new cluster: When you want to configure your local environment to use the cluster: When you want to prolong the life time of a cluster:","title":"The EKSphemeral web UI"}]}